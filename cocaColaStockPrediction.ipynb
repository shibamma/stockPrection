{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "metadata": {
        "id": "YomhB_0hCmqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"Coca-Cola_stock_history.csv\"\n",
        "DATE_COL = \"Date\"\n",
        "TARGET_COL = \"Close\"\n",
        "TEST_SIZE_DAYS = 365\n",
        "RF_SAVE_PATH = \"rf_model.pkl\"\n",
        "LSTM_SAVE_PATH = \"lstm_model.h5\"\n",
        "SCALER_MM_PATH = \"scaler_mm.pkl\"\n",
        "SCALER_STD_PATH = \"scaler_std.pkl\"\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "I4xR1giXC4_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    # ensure date parsing\n",
        "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\n",
        "    df = df.sort_values(DATE_COL).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "LEjVGyVnC-vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineer(df):\n",
        "    \"\"\"\n",
        "    Adds:\n",
        "    - MA_20, MA_50 (moving averages)\n",
        "    - Daily_Return\n",
        "    - Volatility (rolling std of returns over 20 days)\n",
        "    - Year, Month, Day as possible features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['MA_20'] = df[TARGET_COL].rolling(window=20, min_periods=1).mean()\n",
        "    df['MA_50'] = df[TARGET_COL].rolling(window=50, min_periods=1).mean()\n",
        "    df['Daily_Return'] = df[TARGET_COL].pct_change().fillna(0)\n",
        "    df['Volatility'] = df['Daily_Return'].rolling(window=20, min_periods=1).std().fillna(0)\n",
        "    # Optional additional features\n",
        "    df['Year'] = df[DATE_COL].dt.year\n",
        "    df['Month'] = df[DATE_COL].dt.month\n",
        "    df['Day'] = df[DATE_COL].dt.day\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "0W_nITqHDBGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    df = df.copy()\n",
        "    # forward-fill numeric columns, then drop any remaining NA\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numeric_cols] = df[numeric_cols].ffill()\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "rHMIgrZHDDcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split_time(df, test_days=TEST_SIZE_DAYS):\n",
        "    \"\"\"\n",
        "    Splits the dataframe into train/test by time. Test set is last `test_days` rows.\n",
        "    \"\"\"\n",
        "    if test_days <= 0 or test_days >= len(df):\n",
        "        raise ValueError(\"Invalid test_days compared to dataframe length.\")\n",
        "    train = df.iloc[:-test_days].copy()\n",
        "    test = df.iloc[-test_days:].copy()\n",
        "    return train, test\n"
      ],
      "metadata": {
        "id": "qyQIMuAdDFKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RandomForest baseline (tabular features) ---\n",
        "def rf_baseline(train, test, feature_cols, target_col=TARGET_COL):\n",
        "    X_train = train[feature_cols].values\n",
        "    y_train = train[target_col].values\n",
        "    X_test = test[feature_cols].values\n",
        "    y_test = test[target_col].values\n",
        "\n",
        "    # scale features for RF (Standard)\n",
        "    scaler_std = StandardScaler()\n",
        "    X_train_s = scaler_std.fit_transform(X_train)\n",
        "    X_test_s = scaler_std.transform(X_test)\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
        "    rf.fit(X_train_s, y_train)\n",
        "\n",
        "    # predictions\n",
        "    preds = rf.predict(X_test_s)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    # Calculate RMSE manually if 'squared' argument is not supported\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "\n",
        "    # persist\n",
        "    joblib.dump(rf, RF_SAVE_PATH)\n",
        "    joblib.dump(scaler_std, SCALER_STD_PATH)\n",
        "    print(f\"[RF] MAE: {mae:.4f}, RMSE: {rmse:.4f} -- saved to {RF_SAVE_PATH}, {SCALER_STD_PATH}\")\n",
        "    return rf, preds, mae, rmse\n"
      ],
      "metadata": {
        "id": "qGWHdHgBDI63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LSTM model (sequences on scaled close price + features) ---\n",
        "def create_sequences(data_array, seq_len):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data_array) - seq_len):\n",
        "        X.append(data_array[i:i+seq_len])\n",
        "        y.append(data_array[i+seq_len, 0])  # assuming first column is the target (Close)\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "id": "1oeZagBpDMIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_pipeline(train, test, feature_cols, seq_len=60, epochs=30, batch_size=32):\n",
        "    # For LSTM we use MinMaxScaler on the feature set so values are in (0,1)\n",
        "    mm_scaler = MinMaxScaler()\n",
        "    train_vals = train[feature_cols].values\n",
        "    test_vals = test[feature_cols].values\n",
        "    all_vals = np.vstack([train_vals, test_vals])\n",
        "    mm_scaler.fit(all_vals)\n",
        "    train_scaled = mm_scaler.transform(train_vals)\n",
        "    test_scaled = mm_scaler.transform(test_vals)\n",
        "\n",
        "    # Save MinMax scaler\n",
        "    joblib.dump(mm_scaler, SCALER_MM_PATH)\n",
        "\n",
        "    # Create sequences (we'll include all features; target is column 0 -> Close must be first in feature_cols)\n",
        "    train_arr = train_scaled\n",
        "    test_arr = test_scaled\n",
        "\n",
        "    X_train, y_train = create_sequences(train_arr, seq_len)\n",
        "    X_test, y_test = create_sequences(np.vstack([train_arr[-seq_len:], test_arr]), seq_len)\n",
        "\n",
        "    # reshape for LSTM: (samples, timesteps, features)\n",
        "    n_features = X_train.shape[2]\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(seq_len, n_features)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # callbacks\n",
        "    es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "    mc = ModelCheckpoint(LSTM_SAVE_PATH, monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.1,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[es, mc],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on X_test\n",
        "    preds_scaled = model.predict(X_test).flatten()\n",
        "    # preds_scaled are scaled in terms of the target's scale inside MinMaxScaler.\n",
        "    # We need to inverse-transform predictions back to original scale.\n",
        "    # To do that, recreate a \"single-step\" inverse by building arrays where the first column is predicted values\n",
        "    # and the rest are taken from the corresponding last step input (works because MinMax was fit on all features).\n",
        "    def inv_transform_preds(preds, X_reference):\n",
        "        # X_reference: array of shape (n_samples, seq_len, n_features) -- use last time step as context\n",
        "        refs = X_reference[:, -1, :].copy()  # shape (n_samples, n_features)\n",
        "        invs = []\n",
        "        for p, r in zip(preds, refs):\n",
        "            arr = r.copy()\n",
        "            arr[0] = p  # place predicted scaled target into first column\n",
        "            inv = mm_scaler.inverse_transform(arr.reshape(1, -1))[0, 0]  # inverse transform and take target\n",
        "            invs.append(inv)\n",
        "        return np.array(invs)\n",
        "\n",
        "    preds_inv = inv_transform_preds(preds_scaled, X_test)\n",
        "    # Build y_test original-scale values\n",
        "    def inv_transform_targets(y_scaled, X_reference):\n",
        "        refs = X_reference[:, -1, :].copy()\n",
        "        invs = []\n",
        "        for y_s, r in zip(y_scaled, refs):\n",
        "            arr = r.copy()\n",
        "            arr[0] = y_s\n",
        "            inv = mm_scaler.inverse_transform(arr.reshape(1, -1))[0, 0]\n",
        "            invs.append(inv)\n",
        "        return np.array(invs)\n",
        "\n",
        "    y_test_inv = inv_transform_targets(y_test, X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test_inv, preds_inv)\n",
        "    # Calculate RMSE manually if 'squared' argument is not supported\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))\n",
        "\n",
        "    print(f\"[LSTM] MAE: {mae:.4f}, RMSE: {rmse:.4f} -- model saved to {LSTM_SAVE_PATH}\")\n",
        "    # model already saved via ModelCheckpoint\n",
        "    return model, preds_inv, y_test_inv, mae, rmse\n"
      ],
      "metadata": {
        "id": "Uzg0JqD-DVjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(dates, true_vals, preds, title=\"Model predictions vs True\"):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.plot(dates, true_vals, label=\"True\")\n",
        "    plt.plot(dates, preds, label=\"Predicted\")\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(TARGET_COL)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "wKWJT6t_DZIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    #if not os.path.exists(CSV_PATH):\n",
        "        #raise FileNotFoundError(f\"CSV not found at {CSV_PATH}. Please provide the Coca-Cola CSV.\")\n",
        "    df = load_data(CSV_PATH)\n",
        "    df_fe = feature_engineer(df)\n",
        "    df_clean = clean_data(df_fe)\n",
        "\n",
        "    # choose feature list where first column is the target (Close) for LSTM inverse transform convenience\n",
        "    feature_cols = [TARGET_COL, 'Open', 'High', 'Low', 'Volume', 'MA_20', 'MA_50', 'Daily_Return', 'Volatility', 'Year', 'Month', 'Day']\n",
        "\n",
        "    # ensure all feature_cols exist\n",
        "    missing = [c for c in feature_cols if c not in df_clean.columns]\n",
        "    if missing:\n",
        "        raise ValueError(\"Missing expected columns after feature engineering: \" + \", \".join(missing))\n",
        "\n",
        "    train, test = train_test_split_time(df_clean, test_days=TEST_SIZE_DAYS)\n",
        "\n",
        "    # RandomForest baseline on tabular features\n",
        "    rf_features = ['Open', 'High', 'Low', 'Volume', 'MA_20', 'MA_50', 'Daily_Return', 'Volatility']\n",
        "    rf, rf_preds, rf_mae, rf_rmse = rf_baseline(train, test, rf_features)\n",
        "\n",
        "    # Optionally plot RF predictions for the test period\n",
        "    test_dates = test[DATE_COL].iloc[len(test) - len(rf_preds):]  # ensure alignment\n",
        "    try:\n",
        "        plot_results(test_dates, test[TARGET_COL].values, rf_preds, title=\"RandomForest Predictions vs True (Test)\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # LSTM pipeline\n",
        "    # For LSTM we pass feature_cols with TARGET_COL first\n",
        "    lstm_model, lstm_preds, lstm_y_true, lstm_mae, lstm_rmse = lstm_pipeline(train, test, feature_cols, seq_len=60, epochs=25, batch_size=32)\n",
        "\n",
        "    # Plot LSTM results (use last len(preds) dates from test set)\n",
        "    try:\n",
        "        # compute aligned dates for predictions\n",
        "        # After sequence creation test predictions correspond to some subset; we used test sequences from combined tail+test,\n",
        "        # so map predictions to test dataframe's dates conservatively:\n",
        "        aligned_dates = test[DATE_COL].iloc[0:len(lstm_preds)].values\n",
        "        plot_results(aligned_dates, lstm_y_true, lstm_preds, title=\"LSTM Predictions vs True (Test)\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RyLBT2ThDcWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}